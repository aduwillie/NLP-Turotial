{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO4Imy81z88IuFfwiis6Lc7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aduwillie/NLP-Tutorial/blob/main/nlp_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8i6DINWXUpe1",
        "outputId": "f61e6270-563f-4cbd-ae5f-549ba18a3a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.9.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.9.0-cp310-cp310-manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext, torchdata\n",
            "Successfully installed torchdata-0.9.0 torchtext-0.18.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.6)\n",
            "Collecting afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=32cc406cd53f625744aae1ea9e8303361c1283c6cc93e9035512cacc66a24add\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install sklearn\n",
        "!pip install gensim\n",
        "!pip install transformers\n",
        "!pip install torch torchaudio torchvision torchtext torchdata\n",
        "!pip install tensorflow\n",
        "!pip install numpy\n",
        "!pip install unidecode\n",
        "!pip install textblob\n",
        "!pip install afinn\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Text Data\n",
        "\n",
        "This is basically unstructured data and comes in different forms such as articles, social media posts, emails, chat messages etc.\n",
        "\n",
        "## Text Preprocessing\n",
        "\n",
        "This is a crucial step that ensures that text is clean and in the format that can be analyzed by models. This includes:\n",
        "\n",
        "1. Punctuation removal\n",
        "2. Stop word removal\n",
        "3. Non-essential elements - special characters, HTML tags, that do not add any extra value\n",
        "\n",
        "## Standardization\n",
        "\n",
        "This includes converting text to a standardized format. This includes:\n",
        "\n",
        "1. Lowercasing\n",
        "2. Stemming - Reducing word to their base or root forms\n",
        "3. Lemmatization - Similar to stemming. Reduces words to their dictionary or canonical form."
      ],
      "metadata": {
        "id": "oXyOqOweU2y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display text\n",
        "text = \"This is a sample text to play with. My name is Michael Kyle\"\n",
        "print(text)\n",
        "\n",
        "text_len = len(text)\n",
        "print(\"Length of text is:\", text_len)\n",
        "\n",
        "unique_chars = set(text)\n",
        "print(\"Unique chars:\", unique_chars)\n",
        "\n",
        "words = text.split()\n",
        "print(\"Words in text:\", words)\n",
        "\n",
        "words_len = len(words)\n",
        "print(\"Total count of words:\", words_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7wD8B-pV_Eu",
        "outputId": "c0a8bed6-2806-4c4f-d3d5-153ed641f625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sample text to play with. My name is Michael Kyle\n",
            "Length of text is: 59\n",
            "Unique chars: {'i', 'n', 'a', 't', 'K', 'l', 'e', 'h', 'w', 'T', 's', 'p', 'y', '.', 'x', 'o', 'c', 'm', ' ', 'M'}\n",
            "Words in text: ['This', 'is', 'a', 'sample', 'text', 'to', 'play', 'with.', 'My', 'name', 'is', 'Michael', 'Kyle']\n",
            "Total count of words: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges with Text Data\n",
        "\n",
        "There are several challenges with text data that should be considered. There are:\n",
        "\n",
        "1. Ambiguity - Multiple meanings for words\n",
        "2. Variability - Different sentence structures, styles, slang, etc.\n",
        "3. Noisy data - Text may include irrelevant or redundant information\n",
        "4. High dimensionality - Each unique word can be considered a dimension. The vocabulary size can be ver huge.\n",
        "5. Sentiment and subjectivity - Text date often contains subjective information eg. opinions, emotions, etc.\n",
        "6. Context and dependency - A text often requires considering the contextand dependencies between words.\n",
        "7. Language diversity\n",
        "8. Sarcasm and irony"
      ],
      "metadata": {
        "id": "aYZ6PchwXrzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Natural Language Processing (NLP) enables computers to understand human language\"\n",
        "\n",
        "text_lower = text.lower()\n",
        "print(\"Lowercased:\", text_lower)\n",
        "\n",
        "text_no_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(\"No punctuations:\", text_no_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHCjJpR8ZguL",
        "outputId": "75126e23-bfaa-49f2-e602-9ea6c72a7c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased: natural language processing (nlp) enables computers to understand human language\n",
            "No punctuations: Natural Language Processing NLP enables computers to understand human language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleanup\n",
        "\n",
        "This is crucial in the preprocessing pipeline. It involves transforming raw text into a clean and standardized format. It includes:\n",
        "\n",
        "## Stop word removal\n",
        "\n",
        "Common words that cary minimal meaningful information eg. \"the\", \"is\", \"in\", \"and\", etc. Often filtered out to reduce noise. The benefits of this include:\n",
        "\n",
        "1. Dimensionality reduction\n",
        "2. Processing speed\n",
        "3. Improved accuracy - Classification and sentiment analysis improve a lot with stop words removal\n"
      ],
      "metadata": {
        "id": "z3-C4YBlbL-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "text = \"Natural Language Processing (NLP) enables computers to understand a human language!\"\n",
        "tokens = text.split() # In this case, this is word tokens\n",
        "print(\"Oringinal tokens\", tokens)\n",
        "\n",
        "en_stop_words = set(stopwords.words(\"english\"))\n",
        "print(\"English stop words:\", en_stop_words)\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in en_stop_words] # Removes 'a' from the list of tokens\n",
        "print(\"Filtered tokens:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bV2c2UUcM23",
        "outputId": "3e69725a-763f-4ebd-cf92-f0798f13d3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oringinal tokens ['Natural', 'Language', 'Processing', '(NLP)', 'enables', 'computers', 'to', 'understand', 'a', 'human', 'language!']\n",
            "English stop words: {'my', 'should', \"mightn't\", 't', \"won't\", 'its', 'about', 'what', 'only', 'been', 'if', 'no', 'those', 'his', 'off', 'but', \"shouldn't\", 'our', 'there', 'under', 'once', \"shan't\", 'the', \"don't\", 'y', 'such', \"she's\", 'has', 'does', 'd', 'theirs', 'me', 'in', \"needn't\", 'ourselves', 'own', 've', \"doesn't\", 'on', 'these', 'again', 'until', 'didn', 'below', 'him', 'than', 'don', 'myself', \"aren't\", 'herself', 'their', 'here', 'whom', 'yourselves', \"isn't\", 'just', 'to', 'with', 'into', 'she', 'not', 'an', 'having', 'where', 'aren', 'hers', 'your', \"you'll\", 'won', 'from', 'of', \"should've\", 'yourself', 'will', 'or', 'between', \"wasn't\", 'yours', 'am', 'same', 'that', \"weren't\", 'be', 'it', \"hasn't\", 'you', 'isn', 'her', 'other', 'by', 'as', 'are', 'have', 'wouldn', 'needn', 'wasn', \"mustn't\", 'haven', 'while', 'so', 'too', \"that'll\", \"hadn't\", 'for', 'doesn', 'which', 'was', 'and', 'being', 'most', 'itself', 'hasn', 'over', 'up', 'is', 'when', 'shan', 'o', 'mustn', 'them', 'at', 'some', 'they', 'we', 'doing', 'shouldn', 'now', 'weren', 'were', 'i', 'both', \"didn't\", 'a', \"it's\", \"haven't\", 'above', 'few', 'he', 'all', \"you've\", 'themselves', 'can', 're', 'any', 'during', 'nor', 'why', 'because', 'couldn', 'll', 'himself', 'down', 'before', 'how', 'through', 'hadn', 'who', \"you'd\", 'this', 'after', 's', 'further', \"couldn't\", 'did', 'mightn', 'ours', \"wouldn't\", 'against', 'ain', 'do', 'then', 'more', 'each', 'very', 'had', \"you're\", 'out', 'm', 'ma'}\n",
            "Filtered tokens: ['Natural', 'Language', 'Processing', '(NLP)', 'enables', 'computers', 'understand', 'human', 'language!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "This involves reducing words to their base or true form. This is important because of:\n",
        "\n",
        "1. Dimensionality reduction - Reduces the number of unique words\n",
        "2. Improved accuracy - Text classification, search engines, sentiment analysis can standardize.\n",
        "3. Resource efficiency - Size of vocabular is reduced when dealing with large datasets.\n",
        "\n",
        "### Use cases of stemming\n",
        "\n",
        "1. Search engines - When a user inputs a search term, the search engine can find all relevant documents that contain any form of that term. This improves the results and broadens the results.\n",
        "2. Text classification - Stemming reduces the dimensionality of the text. This is a huge benefit of the classification algorithms.\n",
        "3. Sentiment analysis - Different forms of the word do not need to skew  the analysis. Eg, \"happy\", \"happiness\" would all produce the same form and not treated as separate.\n",
        "\n",
        "Note that a big issue with stemming is that it introduces **context ignorance**. Also, the generated stem doesn't match the base form as present in dictionary."
      ],
      "metadata": {
        "id": "-O__Sna1d4U_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import *\n",
        "\n",
        "text = \"Natural Language Processing (NLP) enables computers to understand a human language!\"\n",
        "tokens = text.split() # In this case, this is word tokens\n",
        "print(\"Oringinal tokens\", tokens)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIXe_2uUeU--",
        "outputId": "18b8e29c-4546-438b-e607-10e9b19b499a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oringinal tokens ['Natural', 'Language', 'Processing', '(NLP)', 'enables', 'computers', 'to', 'understand', 'a', 'human', 'language!']\n",
            "Stemmed tokens: ['natur', 'languag', 'process', '(nlp)', 'enabl', 'comput', 'to', 'understand', 'a', 'human', 'language!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "Everything under Stemming applies. The only difference is that lemma is the base form as representied in dictionary (morphological analysis). Another consideration is that this requires knowledge of the word's part of speech (POS). E.g., the word \"saw\" can be noun or verb and lemmatization can distinguish between these use cases."
      ],
      "metadata": {
        "id": "72WJOzwego4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy # Used below under the \"spacy\" section\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "text = \"Natural Language Processing (NLP) enables computers to understand a human language!\"\n",
        "tokens = text.split() # In this case, this is word tokens\n",
        "print(\"Oringinal tokens\", tokens)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens]\n",
        "print(\"Lemmatized tokens\", lemmatized_tokens)\n",
        "\n",
        "# Try Same with spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "print(\"spacy: Lemmatized tokens:\", lemmatized_tokens) # Consider the word \"enables\" -> \"enable\" and \"computers\" -> \"computer\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voJum-GKhJ3e",
        "outputId": "67122a02-4296-4807-c765-dd46b309b631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oringinal tokens ['Natural', 'Language', 'Processing', '(NLP)', 'enables', 'computers', 'to', 'understand', 'a', 'human', 'language!']\n",
            "Lemmatized tokens ['natural', 'language', 'processing', '(nlp)', 'enables', 'computer', 'to', 'understand', 'a', 'human', 'language!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spacy: Lemmatized tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enable', 'computer', 'to', 'understand', 'a', 'human', 'language', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular Expressions\n",
        "\n",
        "This is a powerful tool that is useful in all text processing and data manipulation scenarios. It allows for searching, matching and manipulating text based on a specific pattern. Practical applications include:\n",
        "\n",
        "1. Text search - Finding specific words or phrases in text data.\n",
        "2. Data validation - Verify if string matches pattern eg. email\n",
        "3. Manipulation - Extracting or replacing parts of a string based on a pattern"
      ],
      "metadata": {
        "id": "1KnmxhkAlC1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Natural Language Processing (NLP) enables computers to understand a human language!\"\n",
        "print(\"Text:\", text)\n",
        "\n",
        "pattern = r\"computers\"\n",
        "\n",
        "found = re.search(pattern, text)\n",
        "if found: print(\"The word \\\"computers\\\" was found\")\n",
        "else: print(\"The word \\\"computers\\\" was not found\")\n",
        "\n",
        "# Replace the word \"language\" to speech\n",
        "replace_pattern = r\"language\"\n",
        "target = \"speech\"\n",
        "new_text = re.sub(replace_pattern, target, text)\n",
        "print(\"New text\", new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4UxXlqglteG",
        "outputId": "a4463fb2-7f5d-4cbe-d785-1740c6aff113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Natural Language Processing (NLP) enables computers to understand a human language!\n",
            "The word \"computers\" was found\n",
            "New text Natural Language Processing (NLP) enables computers to understand a human speech!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "This involves breaking down a piece of text into smaller units. The unit is called **token**. The benefits of tokenization includes:\n",
        "\n",
        "1. Simplification - We can focus on the components rather than the whole text\n",
        "2. Standardization - Consistent and uniform representation of the text. Variations would lead to inconsistencies and errors in analysis.\n",
        "3. Feature extraction - These features can be words, phrases and other elements that hold valuable information\n",
        "\n",
        "Types of tokenization include:\n",
        "\n",
        "1. Word tokenization\n",
        "2. Sentence tokenization\n",
        "3. Character tokenization\n",
        "4. *Subword tokenization (Advanced and mostly for Neural Networks)\n",
        "\n",
        "Applications of tokenization varies. Consider the following:\n",
        "\n",
        "1. Word tokenization helps with text classification projects such as spam detetion, topic labeling as well as general organization.\n",
        "2. Word tokenization also helps with sentiment analysis projects by looking for key words.\n",
        "3. Named Entity Recognizer (NER) are good use cases for word tokenization.\n",
        "3. Both word and sentence tokenization are useful in machine translation contexts"
      ],
      "metadata": {
        "id": "WFeiPfE5nfZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\") # NLTK tokenizer. Need for actual tokenization\n",
        "\n",
        "text = \"Natural Language Processing (NLP) enables computers to understand a human language!\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Word tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word tokens:\", word_tokens)\n",
        "\n",
        "en_word_tokens = [token.text for token in doc]\n",
        "print(\"Spacy: Word tokens:\", en_word_tokens)\n",
        "\n",
        "# Sentence tokenization\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"Sentence tokens:\", sentence_tokens)\n",
        "\n",
        "en_sentence_tokens = [sents.text for sents in doc.sents]\n",
        "print(\"Spacy: Sentence tokens:\", en_sentence_tokens)\n",
        "\n",
        "# Character tokenization\n",
        "char_tokens = list(text)\n",
        "print(\"Character tokens:\", char_tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeButo0jo0cN",
        "outputId": "3af0fb57-591a-44da-8f3c-f7c2694847fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'computers', 'to', 'understand', 'a', 'human', 'language', '!']\n",
            "Spacy: Word tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'computers', 'to', 'understand', 'a', 'human', 'language', '!']\n",
            "Sentence tokens: ['Natural Language Processing (NLP) enables computers to understand a human language!']\n",
            "Spacy: Sentence tokens: ['Natural Language Processing (NLP) enables computers to understand a human language!']\n",
            "Character tokens: ['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', '(', 'N', 'L', 'P', ')', ' ', 'e', 'n', 'a', 'b', 'l', 'e', 's', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', ' ', 't', 'o', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 'a', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9cB6GvAbsWBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "The goal of feature engineering is to transform text into numerical representations whiles preserving the underlying meaning and structure.\n",
        "\n",
        "The process involves various techniques such as the following:\n",
        "\n",
        "1. Bag of words\n",
        "2. TF-IDF\n",
        "3. Word Embeddings\n",
        "4. BERT Embeddings"
      ],
      "metadata": {
        "id": "ejHHbUGmsie1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words\n",
        "\n",
        "It converts text into numerical features by treating each document as an unordered collection of words, ignoring grammer, word order, context, but retaining the frequency of words.\n",
        "\n",
        "To achieve this, there are 3 steps involved:\n",
        "\n",
        "1. Tokenize the text eg. word\n",
        "2. Building the vocabulary\n",
        "3. Vectorizing the text\n",
        "\n",
        "Bag of Words and provides the following advantages:\n",
        "\n",
        "1. Simple - Very straighforward to implement\n",
        "2. Efficiency - It processes text relatively quickly\n",
        "3. Baseline - It serves as a baseline for other complex models\n",
        "\n",
        "However, there are disadvantages that should be considered. These include:\n",
        "\n",
        "1. Loss of context - It neglects order and context.\n",
        "2. High dimensionality - Vocabulary size can grow with large data/corpus\n",
        "3. Sparsity - Many elements would map to 0 which results in sparse representations."
      ],
      "metadata": {
        "id": "J0gK-E4ps-8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Implements BOW\n",
        "\n",
        "# Sample corpus eg. a document split into sentences\n",
        "documents = [\n",
        "    \"Natural language processing is fun as it can interpret human language.\",\n",
        "    \"Language models are important in NLP.\",\n",
        "    \"I care about AI but I need the foundations first.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer() # Convert a collection of text documents to a matrix of token counts.\n",
        "matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "bag_of_words = matrix.toarray()\n",
        "print(\"Bag of words matrix\", bag_of_words)\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRQ37F_Dtoyj",
        "outputId": "6c050bc5-02ad-4589-dbfe-8b43b646ace8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of words matrix [[0 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 2 0 1 0 0 1 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0]\n",
            " [1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1]]\n",
            "Vocabulary: ['about' 'ai' 'are' 'as' 'but' 'can' 'care' 'first' 'foundations' 'fun'\n",
            " 'human' 'important' 'in' 'interpret' 'is' 'it' 'language' 'models'\n",
            " 'natural' 'need' 'nlp' 'processing' 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification using Bag of Words\n",
        "\n",
        "This is a simple project where we use Bag of Words (BOW) for feature engineering."
      ],
      "metadata": {
        "id": "DPvW-xjLxHkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB # This is classifier (model) that is based on the Naive Bayes algorithm\n",
        "from sklearn.model_selection import train_test_split # A helper to split input into test and train data sets by fraction\n",
        "from sklearn.metrics import accuracy_score # A metrics function to help verify score of multiclass models\n",
        "\n",
        "documents = [\n",
        "    \"Natural language processing is fun.\",\n",
        "    \"Language models are important in NLP.\",\n",
        "    \"I enjoy learning about artificial intelligence.\",\n",
        "    \"Machine learning and NLP are closely related\",\n",
        "    \"Deep learning is a subset of machine learning.\"\n",
        "]\n",
        "# For the labels\n",
        "# 1 -> NLP-related, 0 -> AI-related\n",
        "labels = [ 1, 1, 0, 1, 0 ]\n",
        "\n",
        "# Get a vectorizer for Bag of Words (BOW)\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "\"\"\"\n",
        "In ML, there are 2 planes to use - X and Y\n",
        "X represents the inputs\n",
        "Y represents the predictions/labels\n",
        "\n",
        "In this example the bow_matrix is the numerical representation of the input text\n",
        "The text data needed to be converted to numerical representation before passing to the model.\n",
        "\"\"\"\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(bow_matrix, labels, test_size = 0.2, random_state = 31)\n",
        "\n",
        "# Initialize classifier (classification model)\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Generate the predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Verify the accuracy of the classifier/model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js5nEupqxSaw",
        "outputId": "939ccb37-483e-4ce8-c6a4-255ef3babe05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Term Frequence - Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "Unlike Bag of Words (BOW), TF-IDF takes into account the importance of each word, as related to the entire text corpus.\n",
        "\n",
        "This method helps in distinguishing the unique aspects of a document, thereby improving the performance of various NLP tasks such as document classification, clustering and information retrieval.\n",
        "\n",
        "Consider the following advantages to TF-IDF:\n",
        "\n",
        "1. Importance weighting - Assigns higher weights to words that are particularly significant to a document while assigning lower weights to common words.\n",
        "2. Reduction of noise - Less important words such as \"the\", \"is\" are assigned lower weights.\n",
        "3. Capture of nuanced representation - TF-IDF captures both frequency of words in a document as well as within the entire corpus\n",
        "4. Effective for large corpus\n",
        "\n",
        "There are downsides to this as well, such as:\n",
        "\n",
        "1. Sparsity - For corpus with large vocabularies, many elements in the feature vectors may end up being zero, wasting storage.\n",
        "2. Context ignorance - It does not capture the semantics of words i.e., it treats each word independently without considering its surrounding words/context."
      ],
      "metadata": {
        "id": "tKKv6L3J01nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus eg. a document split into sentences\n",
        "documents = [\n",
        "    \"Natural language processing is fun as it can interpret human language.\",\n",
        "    \"Language models are important in NLP.\",\n",
        "    \"I care about AI but I need the foundations first.\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer() # Convert a collection of text documents to a matrix of TfIdf features.\n",
        "matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "tf_idf = matrix.toarray()\n",
        "print(\"TF-IDF matrix:\", tf_idf)\n",
        "\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1aVHUIk2xT0",
        "outputId": "b81ad2b4-4408-4b0c-e847-86885ce1cb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix: [[0.         0.         0.         0.29730323 0.         0.29730323\n",
            "  0.         0.         0.         0.29730323 0.29730323 0.\n",
            "  0.         0.29730323 0.29730323 0.29730323 0.45221354 0.\n",
            "  0.29730323 0.         0.         0.29730323 0.        ]\n",
            " [0.         0.         0.42339448 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.42339448\n",
            "  0.42339448 0.         0.         0.         0.32200242 0.42339448\n",
            "  0.         0.         0.42339448 0.         0.        ]\n",
            " [0.35355339 0.35355339 0.         0.         0.35355339 0.\n",
            "  0.35355339 0.35355339 0.35355339 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.35355339 0.         0.         0.35355339]]\n",
            "Vocabulary: ['about' 'ai' 'are' 'as' 'but' 'can' 'care' 'first' 'foundations' 'fun'\n",
            " 'human' 'important' 'in' 'interpret' 'is' 'it' 'language' 'models'\n",
            " 'natural' 'need' 'nlp' 'processing' 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WkMsFR-e32O3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification using TF-IDF\n",
        "\n",
        "This is a simple project where we use Term Frequenct - Inverse Document Frequency (TF-IDF) for feature engineering."
      ],
      "metadata": {
        "id": "D3kqHKDX32hn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "igWYxvTR4D5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB # This is classifier (model) that is based on the Naive Bayes algorithm\n",
        "from sklearn.model_selection import train_test_split # A helper to split input into test and train data sets by fraction\n",
        "from sklearn.metrics import accuracy_score # A metrics function to help verify score of multiclass models\n",
        "\n",
        "documents = [\n",
        "    \"Natural language processing is fun.\",\n",
        "    \"Language models are important in NLP.\",\n",
        "    \"I enjoy learning about artificial intelligence.\",\n",
        "    \"Machine learning and NLP are closely related\",\n",
        "    \"Deep learning is a subset of machine learning.\"\n",
        "]\n",
        "# For the labels\n",
        "# 1 -> NLP-related, 0 -> AI-related\n",
        "labels = [ 1, 1, 0, 1, 0 ]\n",
        "\n",
        "# Get a vectorizer for Bag of Words (BOW)\n",
        "vectorizer = TfidfVectorizer()\n",
        "tf_idf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "\"\"\"\n",
        "In ML, there are 2 planes to use - X and Y\n",
        "X represents the inputs\n",
        "Y represents the predictions/labels\n",
        "\n",
        "In this example the bow_matrix is the numerical representation of the input text\n",
        "The text data needed to be converted to numerical representation before passing to the model.\n",
        "\"\"\"\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(tf_idf_matrix, labels, test_size = 0.2, random_state = 31)\n",
        "\n",
        "# Initialize classifier (classification model)\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Generate the predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Verify the accuracy of the classifier/model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d75250aa-7bd2-4e3c-e2c7-cfe672cb1b02",
        "id": "oHFOqOGg4EKD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings\n",
        "\n",
        "This allows words to be represented as vectors in a continuous vector space. This approach has significant advantages over traditional BOW and TF-IDF. Word embeddings are designed to capture semantic relationships between words, enabling words with similar meaning to have similar representations in the vector space.\n",
        "\n",
        "Here are key concepts around word embeddings:\n",
        "\n",
        "1. Semantic similarity - Eg, \"king\" and \"queen\" might have similar vectors because they often appear in similar contex such as royalty, governance or historical narratives.\n",
        "2. Continuous vector space - Each word is represented as a point in the continuous vector space. Eg, the difference between \"king\" and \"man\" should be the same as \"queen\" and \"woman\"\n",
        "3. Dimensionality reduction - Similar words have map to same point or are very close.\n",
        "4. Transfer learning - Pre-trained word embeddings can be used in other NLP tasks. Same embeddings can be used in machine translation, sentiment analysis and text classification projects.\n",
        "\n",
        "Popular word embeddings are:\n",
        "\n",
        "1. Word2Vec\n",
        "2. Glove"
      ],
      "metadata": {
        "id": "e7M3_idX5E7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec\n",
        "\n",
        "Developed by Google. It comes in 2 main variants:\n",
        "\n",
        "1. Continuous Bag of Words (CBOW) - It takes a set of context words as inputs and attempts to guess the word that is most likely to fit in the middle of these context words. _**This is very effective in identifying words that frequently appear in similar contexts**_.\n",
        "2. Skip-Gram - It predicts the context words, given the target word. It essentially takes a single word as inputand tries to predict the words that are likely to appear around it within a specified a specified window of context. _**This is especially useful for identifying raw words and their context.**_\n",
        "\n",
        "Both variants aim to capture the intricate relationship between words based on their context, thereby enabling more nuanced and sophisticated language models."
      ],
      "metadata": {
        "id": "Qqvqduda7K-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\") # Download the punkt tokenizer\n",
        "\n",
        "# Sample text/corpus\n",
        "text = \"Natural language processing is fun and exciting. Language models are important in NLP. I enjoy learning about artificial intellignence. Machine learning and NLP are closely related. Deep learning is a subset of machine learning\"\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "tokens = [word_tokenize(word) for word in sentences]\n",
        "print(\"Tokens\", tokens)\n",
        "\n",
        "\"\"\"\n",
        "The mode to use is Word2Vec. It accepts a collection of all sentences in the corpora.\n",
        "\n",
        "Dimensionality refers to the number of elements in the word vector.\n",
        "For example, if a word vector has 100 dimensions, it means each word is represented by a vector of 100 numbers.\n",
        "The dimensionality of these vectors is a crucial factor in how well they capture the nuances of word meanings and relationships.\n",
        "\n",
        "The model defined has a vector_size of 100. This is the same of the dimensionality of the vector\n",
        "\n",
        "In Word2Vec, the window size determines how many words before and after the target word are considered as context.\n",
        "For example, with a window size of 2, the context for the word \"dog\" in the sentence\n",
        "\"The quick brown fox jumps over the lazy dog\" would be [\"quick\", \"brown\", \"jumps\", \"over\"].\n",
        "\n",
        "sg is the value for Subsampling\n",
        "This specifies the technique to use\n",
        "1 -> Skip gram i.e., predict context words given target word\n",
        "0 -> or otherwise for CBOW i.e., predict target word given context words\n",
        "\n",
        "min_count=1 means to ignore all words with a total frequency lower than this(1).\n",
        "This setting can be used to remove rare words, by focusing on common words, or preserve them\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create and train a Word2Vec model for the corpus\n",
        "model = Word2Vec(sentences = tokens, vector_size=100, window=5, sg=1, min_count=1, epochs=3)\n",
        "\n",
        "# Test model\n",
        "target = \"language\"\n",
        "vector = model.wv[\"fun\"]\n",
        "print(\"Vector for language:\", vector)\n",
        "\n",
        "similar_words = model.wv.most_similar(target, topn=2)\n",
        "print(\"Similar words\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ27SAi-89N7",
        "outputId": "23ed457c-ea6b-4c8d-bc95-1f70d2967983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens [['Natural', 'language', 'processing', 'is', 'fun', 'and', 'exciting', '.'], ['Language', 'models', 'are', 'important', 'in', 'NLP', '.'], ['I', 'enjoy', 'learning', 'about', 'artificial', 'intellignence', '.'], ['Machine', 'learning', 'and', 'NLP', 'are', 'closely', 'related', '.'], ['Deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning']]\n",
            "Vector for language: [-0.00515624 -0.00666834 -0.00777684  0.00831073 -0.00198234 -0.00685496\n",
            " -0.00415439  0.00514413 -0.00286914 -0.00374966  0.00162143 -0.00277629\n",
            " -0.00158436  0.00107449 -0.00297794  0.00851928  0.00391094 -0.00995886\n",
            "  0.0062596  -0.00675425  0.00076943  0.00440423 -0.00510337 -0.00211067\n",
            "  0.00809548 -0.00424379 -0.00763626  0.00925791 -0.0021555  -0.00471943\n",
            "  0.0085708   0.00428334  0.00432484  0.00928451 -0.00845308  0.00525532\n",
            "  0.00203935  0.00418828  0.0016979   0.00446413  0.00448629  0.00610452\n",
            " -0.0032021  -0.00457573 -0.00042652  0.00253373 -0.00326317  0.00605772\n",
            "  0.00415413  0.00776459  0.00256927  0.00811668 -0.00138721  0.00807793\n",
            "  0.00371702 -0.00804732 -0.00393361 -0.00247188  0.00489304 -0.00087216\n",
            " -0.00283091  0.00783371  0.0093229  -0.00161493 -0.00515925 -0.00470176\n",
            " -0.00484605 -0.00960283  0.00137202 -0.00422492  0.00252671  0.00561448\n",
            " -0.00406591 -0.00959658  0.0015467  -0.00670012  0.00249517 -0.00378063\n",
            "  0.00707842  0.00064022  0.00356094 -0.00273913 -0.00171055  0.00765279\n",
            "  0.00140768 -0.00585045 -0.0078345   0.00123269  0.00645463  0.00555635\n",
            " -0.00897705  0.00859216  0.00404698  0.00746961  0.00974633 -0.00728958\n",
            " -0.00903996  0.005836    0.00939121  0.00350693]\n",
            "Similar words [('artificial', 0.31915077567100525), ('I', 0.1748247593641281)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Vectors for Word Representation (GloVe)\n",
        "\n",
        "Developed by Stanford University. Unlike Word2Vec, which is based on predicting context words, GloVe relies on matrix factorization of word co-occurrence matrices.\n",
        "\n",
        "To break it down, GloVe works this way:\n",
        "\n",
        "1. Building co-occurence matrix - This is a matrix that records how often each pair of words appears together within a certain context window. For example, if our context window is 5 words, the matrix will count how often the word \"cat\" appears within 5 words of \"dog,\" \"fish,\" \"pet,\" etc.\n",
        "2. Calculating co-occurrence probabilities - Once the matrix is built, GloVe calculates the probability of seeing a pair of words together. This probability is based on the counts from the co-occurrence matrix. For instance, the probability that \"cat\" appears near \"dog\" might be higher than \"cat\" near \"fish.\"\n",
        "3. Creating the word embeddings - GloVe aims to find word vectors (embeddings) that can capture the co-occurrence probabilities. It does this by factorizing the co-occurrence matrix. This means GloVe breaks down the large matrix into smaller matrices of lower dimensions, making it easier to work with. The goal is to find word vectors such that the dot product of any two word vectors is proportional to the logarithm of their co-occurrence probability.\n",
        "4. Optimising the word embeddings - GloVe uses an optimization process to adjust the word vectors so that they best represent the relationships captured in the co-occurrence matrix. It minimizes a loss function that measures the difference between the actual co-occurrence probabilities and the probabilities predicted by the dot products of the word vectors.\n",
        "\n",
        "Due to its approach, GloVe is able to capture both local context as well as broader context across the corpus. It thus often lead to a more accurate embeddings for certain tasks."
      ],
      "metadata": {
        "id": "BgLnvKIUWIWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as downloader\n",
        "\n",
        "# Load a pretrained GloVe embeddings\n",
        "model = downloader.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "target = \"food\"\n",
        "vector = model[target]\n",
        "print(\"Vector for language:\", vector)\n",
        "\n",
        "similar_words = model.most_similar(target)\n",
        "print(\"Similar words:\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6dFb5dxXxNo",
        "outputId": "1c1ad41c-0343-4aee-e93c-83528098334f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for language: [-1.6486e-01  9.1997e-01  2.2737e-01 -4.9031e-01 -1.8082e-03 -3.3803e-01\n",
            "  5.7221e-02  1.4601e-01  4.0202e-01 -2.8858e-01 -4.7495e-01 -5.6369e-01\n",
            "  2.7037e-01  5.1702e-01 -1.1241e-01  1.8314e-01  2.2066e-01 -4.8606e-01\n",
            " -8.7284e-01 -6.2587e-02  4.3016e-02  2.3641e-01  5.9705e-01 -3.8640e-01\n",
            " -2.5194e-01  9.6862e-01 -4.3112e-01 -4.8370e-01 -1.1396e+00  9.2425e-02\n",
            " -1.1476e-01 -7.4291e-02 -6.2524e-02 -9.5122e-02 -2.2714e-01  8.8291e-01\n",
            "  3.9978e-01  7.6631e-01 -6.7697e-01 -6.2829e-01 -1.1872e-01 -2.4492e-01\n",
            " -5.8893e-01 -8.5088e-01  1.1107e+00  4.2190e-01 -1.5072e+00 -1.9509e-01\n",
            " -2.6712e-01 -7.0801e-01  5.5075e-01 -4.6929e-02 -2.5203e-01  7.4411e-01\n",
            " -1.8325e-01 -1.4885e+00 -4.6393e-01 -1.0338e-01  2.3525e+00 -1.5421e-01\n",
            "  3.9833e-01  1.5344e-02  8.0708e-02 -2.7373e-01  9.7057e-01 -1.9383e-02\n",
            "  2.0899e-01 -6.4033e-01  9.2509e-01 -4.5371e-01 -7.0564e-01 -1.6033e-01\n",
            " -7.1761e-02  6.2856e-01  3.5732e-01  8.8802e-01 -6.9127e-01  4.9634e-02\n",
            " -9.3347e-01  6.5396e-01  3.7165e-01  5.8363e-02 -1.0152e+00  7.0845e-01\n",
            " -1.3542e+00 -3.6390e-01  2.5994e-01 -1.8260e-01 -9.8930e-01 -4.4699e-01\n",
            "  8.5016e-01  9.4532e-02  3.7019e-01 -5.0354e-01 -1.2083e+00 -3.5776e-01\n",
            "  2.3899e-01 -6.7904e-02  1.5072e+00  6.0889e-01]\n",
            "Similar words: [('foods', 0.7469059824943542), ('supplies', 0.7264691591262817), ('products', 0.7225049138069153), ('meat', 0.7138239145278931), ('supply', 0.6732637882232666), ('feed', 0.670415461063385), ('medicines', 0.6687097549438477), ('meals', 0.6630422472953796), ('coffee', 0.6627735495567322), ('goods', 0.6610530614852905)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bidrectional Encoder Representations from Transformers (BERT)\n",
        "\n",
        "Developed by Google and use Transformer architecture to solve the problem with static embeddings. Unlike Word2Vec and Glove which provides static representations of words that remain unchanged regardless of context, BERT generates context-aware embeddings. The means the representation of the same word can change depending on the its context in a sentence. Eg, \"bank\" will have different embeddings in the contexts of \"river bank\" and \"bank account\".\n",
        "\n",
        "BERT uses 2 main approaches:\n",
        "\n",
        "1. Pre-training - Trained on a large corpususing 2 unsupervised tasks i.e., Masked Language Modeling (MLM) or Next Sentence Prediction (NSP)\n",
        "2. Fine-tuning\n",
        "\n",
        "To give an example: suppose you have a text classification task where you want to classify emails as spam or not, you can start with BERT model that understands language nuances. During the fine-tuning, the model is trained on the labeled data and adjusts its parameters to slighly optimise for this specific task without losing the broad language understanding it gained during the pre-training phase."
      ],
      "metadata": {
        "id": "FiXFaGzNcMdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Import Pytorch for Neural Network-related work\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load a pretrained BERT model and tokenizer\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\") # Model that uses lowercases\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", clean_up_tokenization_spaces=True)\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language processing is fun\"\n",
        "print(\"Text:\", text)\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\") # Ensure that returned tensors are in pytorch format/friendly\n",
        "\n",
        "# Without computing the gradients with backpropagation\n",
        "# Another way to think of this is to not adjust the model weights, from the original training\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "  #print(outputs)\n",
        "\n",
        "\"\"\"\n",
        "The model output has many parameters. One of them is last_hidden_state\n",
        "The last hidden state has the values we want to really output\n",
        "it is a 3 dimensional array (1 x total_size_of_tokens x  total_dimensions)\n",
        "\"\"\"\n",
        "cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "print(\"BERT embedding for text:\", cls_embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1F4FP1vKj39Q",
        "outputId": "8187859a-d582-4a32-f434-820e98078a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Natural Language processing is fun\n",
            "BERT embedding for text: tensor([[-3.1769e-01, -5.6979e-02, -2.9087e-01,  1.6923e-02, -4.7647e-01,\n",
            "         -4.3632e-01,  1.4920e-01,  6.0166e-01, -8.7724e-02, -3.4493e-01,\n",
            "         -3.2336e-01,  3.3781e-02, -2.4300e-01, -2.4146e-03,  2.9958e-01,\n",
            "          5.7035e-03, -3.3114e-01,  2.3717e-01,  1.4185e-01, -1.2382e-01,\n",
            "         -2.4081e-01, -5.9463e-02, -2.3821e-01, -1.5302e-01,  1.4933e-02,\n",
            "         -1.6121e-01,  1.9571e-01, -2.4481e-01,  3.0754e-01, -4.2377e-02,\n",
            "         -1.1753e-01,  6.8966e-02, -1.9826e-01, -2.4759e-01,  3.0578e-01,\n",
            "         -7.2803e-02,  1.2484e-01, -4.4241e-02,  1.2230e-01,  4.0596e-02,\n",
            "         -1.0609e-01,  1.2684e-01,  1.5265e-01,  8.7406e-03, -2.3450e-01,\n",
            "         -2.6255e-01, -2.4217e+00, -4.5520e-02, -4.0230e-01, -2.6297e-01,\n",
            "         -2.9933e-01, -1.1200e-01,  2.8905e-01,  6.4164e-01,  1.9518e-01,\n",
            "          3.1939e-01, -1.0347e-01,  7.0154e-01,  1.7883e-01,  1.6085e-01,\n",
            "          9.7144e-02,  3.1741e-01, -2.7081e-01,  7.1819e-02,  9.6348e-02,\n",
            "          2.2358e-01, -1.1452e-01,  2.1514e-01, -5.9072e-01,  4.6754e-01,\n",
            "         -4.5327e-01, -3.0581e-01,  2.4345e-01,  2.4210e-02, -2.6119e-01,\n",
            "         -3.8037e-01,  1.2396e-01,  1.6772e-01, -3.0679e-01, -2.7353e-01,\n",
            "         -9.3874e-02,  4.6510e-01,  2.5695e-01, -1.7576e-01,  4.5560e-01,\n",
            "          3.4336e-01, -2.8331e-01,  3.3568e-02,  2.7180e-01,  6.9439e-01,\n",
            "         -5.5689e-02,  6.5167e-02,  2.0671e-01,  2.1851e-01,  5.2141e-01,\n",
            "         -5.2457e-02,  6.6551e-02, -8.1029e-02,  2.0227e-01,  2.6334e-01,\n",
            "         -8.8775e-03, -4.7941e-01,  2.6083e-01, -2.6358e-01, -5.7044e-02,\n",
            "         -2.6498e-01, -7.0923e-02,  3.3923e-02,  1.3841e-01, -3.0372e+00,\n",
            "          3.2462e-02, -1.7898e-01, -2.2535e-01,  9.8784e-02, -2.4788e-01,\n",
            "          7.3002e-01,  4.4701e-01,  4.9251e-01,  1.9348e-01,  1.2942e-01,\n",
            "         -1.7702e-01,  3.4897e-01, -2.2478e-01, -1.6869e-01, -5.2330e-04,\n",
            "          2.2858e-01,  9.8684e-02,  9.6075e-02,  2.1149e-01, -2.1312e-01,\n",
            "          8.4670e-02,  4.8931e-01,  3.6995e-01, -2.9785e-02, -1.2129e-01,\n",
            "         -1.8033e-01,  4.4844e-01,  5.5698e-02, -2.4063e-01,  1.1801e-01,\n",
            "         -4.1891e-01, -2.9087e-01, -3.2090e+00, -1.3910e-01,  5.2266e-01,\n",
            "         -1.5378e-01, -2.0583e-01, -2.2042e-01,  1.7756e-01, -8.1226e-03,\n",
            "          3.7484e-03,  1.1960e-01, -2.0393e-01,  3.1329e-03, -2.6794e-01,\n",
            "          1.5492e-01,  5.0373e-02, -2.8784e-01,  3.0968e-01,  2.3129e-01,\n",
            "         -2.4255e-01, -3.6608e-01, -1.4576e-01, -5.1328e-01, -2.1683e-01,\n",
            "         -9.1936e-02,  4.7027e-01,  1.0860e-01,  1.1143e-01,  9.2957e-02,\n",
            "         -3.1056e-01, -8.0426e-02,  2.1620e-01,  1.6847e-03,  2.5746e-01,\n",
            "         -3.0676e-01,  3.3730e-02,  1.2268e-01,  2.1696e-01, -7.2763e-02,\n",
            "         -1.3370e-01,  6.5675e-01,  9.4488e-02,  1.0596e-02,  3.3289e-01,\n",
            "         -1.2974e-01,  2.9271e-01, -2.5963e-01, -1.1112e-01,  9.7719e-02,\n",
            "         -1.1823e-01,  1.1342e-01,  3.7978e-01, -8.6935e-03,  3.9174e-01,\n",
            "          6.1449e-02,  2.3554e-01, -4.1087e-01,  4.6733e-02,  3.8339e-01,\n",
            "         -3.1406e-02, -1.5543e-01, -3.5502e-01, -7.0297e-02, -2.3907e-02,\n",
            "          3.8721e+00,  1.5581e-01,  1.9339e-02,  1.1417e-01, -2.5765e-02,\n",
            "          6.8252e-02,  1.4540e-01,  1.4957e-01, -4.5746e-01,  4.4644e-01,\n",
            "          9.8878e-02,  2.2648e-01,  1.9132e-01, -5.9424e-02, -2.0535e-01,\n",
            "          1.1682e-01,  1.5569e-01, -1.1300e-01,  1.7251e-02, -1.9286e-01,\n",
            "          1.9737e-01,  3.9778e-01,  3.9347e-01,  2.3371e-02, -1.0693e+00,\n",
            "          5.6873e-02, -1.2062e-01, -1.0446e-01,  9.5476e-02, -2.7278e-01,\n",
            "         -2.7238e-01, -1.5172e-01, -4.2357e-01,  7.9287e-02,  2.0875e-01,\n",
            "          1.4759e-02,  3.1969e-01, -1.7864e-02, -1.5778e-02, -1.0123e-02,\n",
            "         -5.9149e-03,  1.5014e-01, -4.2083e-02,  3.6309e-01, -5.6606e-02,\n",
            "          1.2877e-01,  3.2991e-02,  1.7175e-02, -2.3349e-01,  1.2368e-01,\n",
            "         -4.4188e-02,  9.9556e-02,  4.2445e-01, -2.4911e-01,  2.6160e-01,\n",
            "          3.0038e-03, -1.3438e-01,  3.9614e-03,  9.3956e-02, -4.2283e-01,\n",
            "         -2.6396e-01, -1.8435e-01, -4.9656e-02,  2.1306e-01,  1.7192e-01,\n",
            "          3.5965e-01, -4.2347e-01, -2.2808e-01, -4.2449e+00,  2.6198e-01,\n",
            "         -1.1055e-01,  3.3083e-01,  2.1491e-01, -2.4742e-01,  1.4476e-01,\n",
            "          1.1683e-01,  2.3285e-01, -2.4983e-01,  1.7018e-01, -2.6917e-02,\n",
            "          7.0128e-02,  1.7341e-01, -5.1653e-01,  7.3714e-01, -6.0542e-02,\n",
            "         -1.1864e-01, -1.2945e-01, -3.7647e-02,  1.7805e-01,  2.7770e-01,\n",
            "         -1.7130e-01, -1.3616e-01,  1.1514e-01, -3.7826e-01, -1.8644e-01,\n",
            "         -2.2125e-01,  2.1800e-03, -1.4183e-01, -1.0235e-01, -1.3274e-01,\n",
            "         -1.3580e-01, -1.0163e-01, -1.0250e-02, -2.6010e+00,  1.9582e-01,\n",
            "          1.7337e-01, -2.7747e-01, -1.4718e-02, -1.6297e-01,  2.4654e-01,\n",
            "         -1.3406e-01, -2.4772e-01,  1.2798e-01,  4.4606e-02, -9.0982e-02,\n",
            "         -2.8451e-01,  1.3596e-01,  4.6459e-01,  2.8324e-02,  4.0692e-01,\n",
            "          6.5948e-02,  1.6486e-01,  9.2858e-02,  1.5396e-01,  2.1558e-01,\n",
            "          4.3735e-02, -1.6812e-01,  4.2078e-02,  2.8173e-01, -2.2934e-01,\n",
            "         -1.5963e-01, -1.6039e-01, -1.4255e-01, -1.2253e-02,  1.7106e-01,\n",
            "          8.9543e-02, -1.4915e-01, -3.9705e-01, -1.9098e-01,  6.4794e-01,\n",
            "          2.5122e-01,  6.1626e-01, -4.4265e-02, -5.5497e-02,  6.6384e-01,\n",
            "          3.8644e-01,  1.2316e-01,  2.5274e-01, -1.7965e-01,  1.3243e-02,\n",
            "         -5.5080e-02,  1.2365e-01, -2.1600e-01, -4.2965e-01, -1.7311e-02,\n",
            "          1.0869e+00, -5.2878e-02,  2.9706e-01, -2.2390e-01,  4.7023e-01,\n",
            "          1.6704e-01,  3.8517e-01,  1.2377e-01,  7.0925e-01,  2.0429e-02,\n",
            "          4.4408e-01, -1.9645e-01, -5.4615e-02, -2.8611e-01,  1.3735e-01,\n",
            "         -1.9859e-01,  5.7519e-02,  3.1069e-01,  1.1010e-01,  4.4268e-01,\n",
            "          1.8934e-02, -7.8516e-01, -1.3300e-01, -2.1531e-01, -3.4451e-01,\n",
            "          8.3122e-02,  1.3414e-01, -3.5675e-02, -1.6089e-01, -1.1765e-01,\n",
            "         -2.6485e-01,  3.1909e-01, -4.3384e-02, -3.4423e-02, -1.9560e-01,\n",
            "          2.6452e-01, -5.8563e-01,  1.7541e-01, -2.5280e-01,  3.2327e-01,\n",
            "         -9.0754e-04,  2.1818e-02, -5.5169e-02, -2.3370e-01,  4.1825e-01,\n",
            "         -7.5906e-01,  2.6344e-01, -4.8615e-01, -8.5762e-02, -5.6193e-01,\n",
            "         -6.8306e-01,  3.8334e-01, -2.3569e-02, -5.2769e-02, -2.5945e-01,\n",
            "          3.2666e-01,  4.9797e-01,  2.6917e-01, -1.5296e-01,  4.3161e-01,\n",
            "          7.4712e-02,  9.6968e-02,  6.5186e-01,  3.6689e-01, -3.8275e-01,\n",
            "          3.3240e-01,  1.2335e-01, -6.9139e-02,  2.7801e-01,  2.9361e-01,\n",
            "          1.3314e-02, -1.3492e-01, -7.6860e-02,  1.0375e-01,  1.2109e-01,\n",
            "         -1.7196e-01, -5.7706e-01, -1.4814e-01,  8.5086e-02, -7.5922e-02,\n",
            "         -2.3080e-02, -5.6675e-01, -1.8550e-02, -3.4776e-01,  2.0157e-01,\n",
            "          7.5050e-01,  4.0172e-01, -1.4313e-01,  3.8087e-01, -2.2120e-01,\n",
            "         -4.7274e-02,  3.2734e-01,  8.7693e-02,  4.5106e-01,  1.5375e-01,\n",
            "         -3.2343e-02,  5.6112e-03,  2.7407e-01,  2.2384e-01, -5.8659e-01,\n",
            "         -6.8539e-02, -7.0491e-02,  4.7618e-01, -1.1964e-01, -4.8610e-02,\n",
            "          1.2008e-01, -1.0700e-02, -6.0561e-03,  3.2293e-01,  9.9073e-02,\n",
            "         -1.2228e+00,  2.8496e-01,  3.6224e-01, -1.9257e-02, -2.7657e-02,\n",
            "          1.0728e-01, -3.0441e-01,  4.7094e-01,  6.1135e-02, -2.7010e-01,\n",
            "         -3.2298e-01,  6.7590e-02,  1.5449e-01,  1.4355e-02,  1.5415e-01,\n",
            "         -2.0699e-01, -1.6584e-02, -2.4694e-01,  3.6288e-01, -1.8369e-02,\n",
            "         -1.8615e-01,  5.1184e-01, -2.0000e-01, -3.2069e-01,  3.1460e-01,\n",
            "         -1.8385e-02, -3.8303e-01,  5.8298e-01,  2.9415e-01,  4.0184e-01,\n",
            "          1.6021e-01, -3.9994e-01, -1.7009e-01,  5.8196e-02, -8.7418e-02,\n",
            "          6.7374e-02, -8.4149e-02, -3.7781e-01,  4.7028e-01,  1.9965e-01,\n",
            "         -1.5772e-01,  2.1191e-01,  1.6867e-01, -1.6519e-01, -1.2233e-02,\n",
            "          1.8034e-01, -5.4758e-01,  2.3382e-01, -1.1071e-01, -2.2520e-01,\n",
            "          1.5054e-02, -4.1828e-01, -3.6825e-01, -6.0712e-02,  3.3261e-01,\n",
            "         -1.0025e-01, -1.7570e-01, -1.0289e-01, -2.4117e-01, -4.1131e-02,\n",
            "         -4.6301e-02, -2.0825e-01, -2.2740e-01, -3.6387e-02, -5.7544e-01,\n",
            "         -8.6308e-01,  4.0275e-01, -1.2453e-01, -6.1320e-02, -3.1381e-02,\n",
            "          2.6559e-01, -3.3271e-02, -2.4469e-01,  1.1270e-01, -2.5816e-01,\n",
            "          4.2121e-01, -2.6459e-01, -3.7182e-01,  4.7273e-01, -4.4883e-02,\n",
            "         -6.4730e-03, -1.0123e-01, -1.5997e-01,  1.3895e-01,  2.6385e-01,\n",
            "         -5.5566e-02, -3.6428e-01, -1.0929e-01, -2.3230e-01, -5.0819e-02,\n",
            "         -3.0720e-01, -3.7382e-01,  3.1455e-01,  2.9328e-01,  2.9896e-01,\n",
            "          2.0211e-02,  1.2201e-01,  7.2556e-03, -1.8918e-03,  7.3074e-02,\n",
            "         -4.8200e-01,  2.2899e-01,  1.9580e-01,  2.9071e-01,  1.8953e-01,\n",
            "          4.3026e-01,  5.1534e-01,  1.5906e-01, -1.3234e-01, -2.5101e-01,\n",
            "         -1.6044e-01,  2.2366e-01,  1.2907e-01, -1.9882e-01,  5.0515e-02,\n",
            "          1.5005e-01,  2.6494e-01, -1.9806e-01,  2.3751e+00,  2.7516e-01,\n",
            "          2.7965e-01, -3.1199e-01,  2.3966e-01, -1.6523e-01,  4.2684e-01,\n",
            "         -2.1618e-02, -2.2427e-01,  2.3418e-01, -1.5442e-01,  1.4744e-02,\n",
            "          1.7744e-01,  5.2555e-02,  2.9969e-01,  9.6178e-02, -3.4260e-01,\n",
            "         -1.9447e-01, -5.2602e-01, -1.8710e-01, -2.2735e-01,  4.4708e-01,\n",
            "         -9.6221e-02, -1.2818e-01,  2.4962e-01,  1.4374e-01,  3.0381e-01,\n",
            "          2.4234e-02,  9.4072e-03, -3.0104e-01, -2.3845e-01,  2.7173e-01,\n",
            "          9.2137e-02,  7.4347e-02, -8.3629e-02,  7.9699e-02,  2.8274e-01,\n",
            "         -2.5463e-01, -6.9115e-02,  4.5397e-01, -8.3369e-02, -6.6707e-01,\n",
            "          4.0403e-01,  2.3905e-01, -1.1348e-01,  6.9955e-01, -2.2177e-01,\n",
            "         -5.2629e-01,  1.9388e-01, -1.0608e-02,  2.5454e-01,  3.2458e-01,\n",
            "         -4.2157e-01,  3.5791e-02, -1.5977e-01, -3.5390e-01,  3.0581e-01,\n",
            "         -4.1930e-03, -1.1369e-01,  4.2546e-01,  4.6132e-01,  2.6962e-01,\n",
            "          1.2629e-01, -1.3810e-01, -4.8343e-01,  5.0031e-02, -3.2825e-01,\n",
            "          5.7254e-01, -2.2943e-01,  7.4860e-02,  3.7843e-01,  1.3407e-01,\n",
            "          3.1028e-01,  1.3833e-01,  3.9421e-01, -1.6307e-01,  2.6756e-01,\n",
            "          3.3976e-02, -3.9847e-02, -3.1636e+00,  1.1197e-01,  1.1469e-01,\n",
            "          2.9901e-01, -1.7849e-02,  4.9660e-01,  2.8163e-01, -8.3151e-02,\n",
            "         -4.7344e-02, -3.1218e-01,  2.0344e-01,  1.1319e-02,  3.8743e-01,\n",
            "          6.3542e-01, -2.9278e-03,  1.1473e-01, -7.7376e-02, -4.8569e-01,\n",
            "          1.1407e-01, -2.0327e-01, -3.4596e-01,  5.1722e-02, -4.3435e-02,\n",
            "          1.1773e-01, -3.8044e-01,  1.1925e-01,  1.5509e-01, -1.6563e-01,\n",
            "          1.4817e-01, -4.5910e-02,  1.0416e-02,  9.3371e-02,  9.1239e-02,\n",
            "          1.6076e-01, -1.1596e-01, -2.1884e-01, -3.4910e-02, -3.2454e-01,\n",
            "          2.2751e-01,  6.4266e-02, -3.2080e-01,  4.8018e-01, -2.9850e-02,\n",
            "          3.0758e-01, -5.4971e-01, -4.4966e-01,  2.1814e-01, -1.6707e-01,\n",
            "          3.9182e-01, -2.5861e-01, -9.3182e-02,  2.3159e-01, -1.2953e-01,\n",
            "          1.8518e-02,  2.2499e-01,  5.3038e-02,  5.1350e-01, -4.9537e-02,\n",
            "         -1.1603e-01,  2.1432e-02,  1.1320e-01, -1.0656e-01,  2.4800e-02,\n",
            "          1.4371e-01,  2.0820e-01, -3.5066e-01,  1.6410e-01,  1.9123e-01,\n",
            "         -9.9964e-02, -2.9342e-01, -5.1763e-02, -1.3143e-01,  5.8993e-01,\n",
            "          3.0429e-02,  2.1246e-01,  2.7858e-01,  1.4917e-01,  8.1722e-02,\n",
            "          1.9931e-01,  3.2347e-01, -2.6133e-02, -3.8559e-01, -4.3800e-01,\n",
            "         -5.3105e-02,  4.6346e-01, -8.4036e+00, -1.6679e-01, -3.1838e-01,\n",
            "         -2.7756e-01,  5.8909e-03, -3.9264e-01,  1.1765e-01,  6.9603e-02,\n",
            "          1.0139e-01,  2.4240e-01, -1.1692e-01,  3.5002e-01,  2.3544e-03,\n",
            "         -2.7452e-01, -8.3264e-02,  6.8710e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Modeling\n",
        "\n",
        "This involves predicting the next word or sequence of words in a sentence. It serves as the backbone for speech recognition, maching translation, text generation and more."
      ],
      "metadata": {
        "id": "WJpD4sdjtqe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-grams\n",
        "\n",
        "N-grams are contiguous sequence of N items derived from a given sample of text or speech. N can be 1, 2, 3, etc.\n",
        "\n",
        "There are challenges with N-grams such as:\n",
        "\n",
        "1. Sparsity - As N increases, many N-grams may not appear in the training corpus, making it difficult to estimate their possibilities accurately.\n",
        "2. Context limitation - N-grams only capture a fixed window of context, which may be insufficient for capturing long-rang dependencies in a language.\n",
        "3. Memory usage - storage related problems\n",
        "4. Inability to capture semantic meaning\n",
        "\n",
        "Despite these challenges, n-grams are applicable in the following areas:\n",
        "\n",
        "1. Text prediction\n",
        "2. Speech recognition - we can filter our improbable word combinations\n",
        "3. Machine translations\n",
        "4. Text generation"
      ],
      "metadata": {
        "id": "dp-qRzWYt9BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "# Download the punkt tokenizer\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural language processing is a fascinating field to study\"\n",
        "print(\"Text:\", text)\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\",tokens)\n",
        "\n",
        "def generate_ngrams(tokens, n):\n",
        "  n_grams = ngrams(tokens, n)\n",
        "  return [' '.join(grams) for grams in n_grams]\n",
        "\n",
        "print(\"Unigrams:\", generate_ngrams(tokens, 1))\n",
        "print(\"Biigrams:\", generate_ngrams(tokens, 2))\n",
        "print(\"Trigrams:\", generate_ngrams(tokens, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpL1Aa7euMpD",
        "outputId": "3fed778e-ea12-4667-8090-d9ecec7d5a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Natural language processing is a fascinating field to study\n",
            "Tokens: ['Natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field', 'to', 'study']\n",
            "Unigrams: ['Natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field', 'to', 'study']\n",
            "Biigrams: ['Natural language', 'language processing', 'processing is', 'is a', 'a fascinating', 'fascinating field', 'field to', 'to study']\n",
            "Trigrams: ['Natural language processing', 'language processing is', 'processing is a', 'is a fascinating', 'a fascinating field', 'fascinating field to', 'field to study']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training an N-Gram Model"
      ],
      "metadata": {
        "id": "Lx6MtIadwohE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import bigrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Sample corpus\n",
        "documents = [\n",
        "    \"Natural language processing is fun.\",\n",
        "    \"Language models are important in NLP.\",\n",
        "    \"I enjoy learning about artificial intelligence.\",\n",
        "    \"Machine learning and NLP are closely related\",\n",
        "    \"Deep learning is a subset of machine learning.\"\n",
        "]\n",
        "\n",
        "tokens = [word_tokenize(sentence) for sentence in documents]\n",
        "print(\"Tokens\", tokens)\n",
        "\n",
        "# Define model\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count bigrams\n",
        "for sentence in tokens:\n",
        "  bi_grams = list(bigrams(sentence))\n",
        "  for w1, w2 in bi_grams:\n",
        "    model[w1][w2] += 1\n",
        "\n",
        "# Calculate probabilities\n",
        "for w1 in model:\n",
        "  total = float(sum(model[w1].values()))\n",
        "  for w2 in model[w1]:\n",
        "    model[w1][w2] /= total\n",
        "\n",
        "\n",
        "def get_bigram_prob(w1, w2):\n",
        "  return model[w1][w2]\n",
        "\n",
        "print(\"Prob:\", get_bigram_prob(\"and\", \"NLP\")) # Add or remove a new char\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxLUy1iJws_G",
        "outputId": "3d1b55ae-2565-45ad-9fdd-d62297abb3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens [['Natural', 'language', 'processing', 'is', 'fun', '.'], ['Language', 'models', 'are', 'important', 'in', 'NLP', '.'], ['I', 'enjoy', 'learning', 'about', 'artificial', 'intelligence', '.'], ['Machine', 'learning', 'and', 'NLP', 'are', 'closely', 'related'], ['Deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning', '.']]\n",
            "Prob: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks (RNN)\n",
        "\n",
        "This is a subclass of neural networks. They are designed for sequential data. The ability to remember past inputs makes RNNs well-suited for a wide range of tasks that involve time seres data or natural language where order is preserved. They are adept at problems where sequential order of data is important, such as speech recognition, video analysis, financial forecasting.\n",
        "\n",
        "One of the biggest problems with RNNs is the vanishing gradient problem. Imagine you're playing a game of \"telephone\" with a long line of friends. In this game, you whisper a message to the first person, they whisper it to the next person, and so on. The goal is to see if the last person hears the same message you started with.\n",
        "\n",
        "If each person whispers very softly, the message gets quieter and quieter.\n",
        "\n",
        "By the time it reaches the last person, the message might be so quiet that its almost impossible to hear anything.\n",
        "\n",
        "This is similar to what happens in Recurrent Neural Networks (RNNs) when they're learning from data"
      ],
      "metadata": {
        "id": "DNxfIfH2zsgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Sample text\n",
        "text = \"hello world\"\n",
        "\n",
        "# Character level vocabulary\n",
        "chars = sorted(set(text))\n",
        "char_to_idx = { char:idx for idx, char in enumerate(chars) }\n",
        "idx_to_char = { idx:char for char, idx in char_to_idx.items() }\n",
        "\n",
        "\"\"\"\n",
        "We want to generate input/output pairs\n",
        "Eg, give the model \"hel\" and tell it to generate the next character tokens\n",
        "\n",
        "To train the model, we need to plot the 3 char inputs to output chars\n",
        "X = input chars\n",
        "y = output chars\n",
        "\"\"\"\n",
        "seq_n = 3 # Expect 3 chars to be input\n",
        "X = [] # Start empty\n",
        "y = [] # Start empty\n",
        "\n",
        "\"\"\"\n",
        "The goal is to generate a map\n",
        "Example:\n",
        "h -> ell\n",
        "e -> llo\n",
        "l -> lo\n",
        "\"\"\"\n",
        "for i in range(len(text) - seq_n):\n",
        "  X.append([char_to_idx[char] for char in text[i:i + seq_n]])\n",
        "  y.append(char_to_idx[text[i + seq_n]])\n",
        "\n",
        "X = np.array(X)\n",
        "y = to_categorical(y, num_classes=len(chars))\n",
        "\n",
        "# Reshape input\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Define the RNN Model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(50, input_shape=(seq_n, 1)))\n",
        "model.add(Dense(len(chars), activation=\"softmax\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=10, verbose=1)\n",
        "\n",
        "def generate_text(start_str, num_generate):\n",
        "  input_eval = [char_to_idx[s] for s in start_str]\n",
        "  input_eval = np.array(input_eval).reshape((1, len(input_eval), 1))\n",
        "\n",
        "  text_gen = []\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    preds = model.predict(input_eval)\n",
        "    pred_id = np.argmax(preds[-1])\n",
        "\n",
        "    input_eval = np.append(input_eval[:, 1:], [[pred_id]], axis=1)\n",
        "    text_gen.append(idx_to_char[pred_id])\n",
        "\n",
        "  return start_str + ''.join(text_gen);\n",
        "\n",
        "# Generate some text\n",
        "start_str = \"hel\"\n",
        "gen_text = generate_text(start_str, 5)\n",
        "print(\"Generated:\", gen_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "F04rxom72GpG",
        "outputId": "52bf8f18-4c81-4be4-80f2-5f5499720db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 2.3599\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.2643\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.1769\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 2.0977\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.0263\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.9621\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.9042\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.8517\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.8038\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.7596\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-d39c1de708e3>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Generate some text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mstart_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hel\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mgen_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-d39c1de708e3>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(start_str, num_generate)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mpred_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0minput_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mtext_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5616\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5617\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5618\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Applications\n",
        "\n",
        "1. One to one - Image classification\n",
        "2. One to many - Image captioning. A single image is fed but we output a sequence of characters describing the image\n",
        "3. Many to one - Sentiment classification\n",
        "4. Many to many - Machine translation\n",
        "5. *Many to many - Video translation\n",
        "\n",
        "Note that the biggest pro of this model is that computation takes into account historical data. However, it can lost information from a long time ago."
      ],
      "metadata": {
        "id": "HKrG-ytO9DJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape) # (60k, 28, 28)\n",
        "# This is a sequence, each row is part of a seq\n",
        "print(x_train[0].shape) # (28,28)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "model = Sequential()\n",
        "# Only return True for sequences if going down to a RNN and not a Dense\n",
        "model.add(LSTM(128, input_shape=(x_train.shape[1:]), activation=\"relu\", return_sequences=True))\n",
        "model.add(Dropout(0.2)) # 20% dropout\n",
        "\n",
        "model.add(LSTM(128, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SyIgjkAAVdH",
        "outputId": "8cecdb96-6569-4268-ca6d-92c463db721b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(28, 28)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 103ms/step - accuracy: 0.5658 - loss: 1.2381 - val_accuracy: 0.9554 - val_loss: 0.1445\n",
            "Epoch 2/3\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 101ms/step - accuracy: 0.9499 - loss: 0.1819 - val_accuracy: 0.9707 - val_loss: 0.1041\n",
            "Epoch 3/3\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 101ms/step - accuracy: 0.9700 - loss: 0.1148 - val_accuracy: 0.9819 - val_loss: 0.0672\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a45aee73d30>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN using Pytorch"
      ],
      "metadata": {
        "id": "HYQJEKMhOzx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "## Download data file if not exists\n",
        "if not os.path.isdir(\"/content/data\"):\n",
        "  print(\"Downloading data and unzipping\")\n",
        "  !wget https://download.pytorch.org/tutorial/data.zip\n",
        "  !unzip *.zip\n",
        "else:\n",
        "  print(\"No need to download data\")\n",
        "\n",
        "# Helpers\n",
        "def unicode_to_ascii(input: str) -> str:\n",
        "  return unidecode(str)\n",
        "\n",
        "def get_country_from_filename(f: str) -> str:\n",
        "  # Another alterntive is to use regular expression to extract name before .txt\n",
        "  return f.split(\".\")[0]\n",
        "\n",
        "# Prepare\n",
        "letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(letters)\n",
        "print(\"All letters:\", n_letters)\n",
        "\n",
        "all_names = []\n",
        "all_country = []\n",
        "\n",
        "\"\"\"\n",
        "Pull all the names and country from the file\n",
        "This is the expected output:\n",
        "\n",
        "all_names = [n1, n2, n3]\n",
        "all_country = [c1, c1, c2]\n",
        "\n",
        "Goal is to get the same size for names and countries (x -> y mapping)\n",
        "\"\"\"\n",
        "for f in os.listdir(\"/content/data/names\"):\n",
        "  fl = open(\"/content/data/names/\" + f, \"r\")\n",
        "  lines = [line.strip() for line in fl.readlines()]\n",
        "  cleaned_list = list(map(unidecode, lines))\n",
        "  n_names_in_file = len(cleaned_list)\n",
        "  all_names.extend(cleaned_list)\n",
        "\n",
        "  country_name = get_country_from_filename(f)\n",
        "  all_country.extend([country_name] * n_names_in_file)\n",
        "\n",
        "n_rows = len(all_names)\n",
        "print(\"Rows to process:\", n_rows)\n",
        "print(\"All names count:\", len(all_names))\n",
        "print(\"All countries count\", len(all_country))\n",
        "\n",
        "# Prepare the inputs for processing\n",
        "# Start with one-hot encoding\n",
        "emb = torch.eye(n_letters) # Represent each char with a one-hot encoding\n",
        "print(\"One-hot encoding:\", emb)\n",
        "print(\"Shape of embedding:\", emb.shape) # 57 unique chars\n",
        "\n",
        "# Create a country to index map eg. { french: 0, english: 1, ... }\n",
        "unique_countries = np.unique(all_country)\n",
        "mapping = dict(zip(np.unique(all_country), range(n_rows)))\n",
        "print(\"Country to index map:\", mapping)\n",
        "\n",
        "# Get data from an index, from names and country list\n",
        "def get_data(idx):\n",
        "  name = all_names[idx]\n",
        "  country = all_country[idx]\n",
        "\n",
        "  char_list_for_name = np.array(list(name)) # Convert name to numpy\n",
        "  \"\"\"\n",
        "  There are 2 sides\n",
        "  (1)char_list_for_name -> ['a', 'b', 'c']\n",
        "  char_list_for_name[..., None] -> [['a'], ['b'], ['c']] # Trick to reshape\n",
        "  (2) np.array(list(letters)) -> ['a', 'b', 'c']\n",
        "\n",
        "  The np.where performs a comparison where each char on the left is compared to all letters\n",
        "  [[ True, False, False, ..., False],  # 'a' compared with 'a', 'b', 'c', ...\n",
        "  [False,  True, False, ..., False],  # 'b' compared with 'a', 'b', 'c', ...\n",
        "  [False, False,  True, ..., False]]  # 'c' compared with 'a', 'b', 'c', ...\n",
        "\n",
        "  np.where returns a tuple of row indices and column indices\n",
        "  [1] corresponds to the column indices\n",
        "  \"\"\"\n",
        "  indices = np.where(char_list_for_name[..., None] == np.array(list(letters)))[1]\n",
        "  return emb[torch.from_numpy(indices)], torch.tensor(mapping[country]) # name (X) to country (y)\n",
        "\n",
        "\n",
        "# print(get_data(0)) # [A, b, l] -> [26, 1, 11]\n",
        "\n",
        "# Define the RNN model\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, n_country, n_letters):\n",
        "    super(RNN, self).__init__()\n",
        "    self.rnn = nn.RNN(input_size=n_letters, hidden_size=2 * n_letters) # 57 (letters), hidden is 57 * 2 (whatever)\n",
        "    self.fc = nn.Linear(2 * n_letters, n_country)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, _ = self.rnn(x)\n",
        "    out1 = self.fc(out[-1, :])\n",
        "    return out1\n",
        "\n",
        "model = RNN(len(np.unique(all_country)), n_letters)\n",
        "print(model)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define epochs\n",
        "n_epochs = 10\n",
        "all_losses = []\n",
        "for epoch in range(n_epochs):\n",
        "  arr = np.arange(n_rows)\n",
        "  np.random.shuffle(arr)\n",
        "\n",
        "  epoch_loss = 0\n",
        "  for idx in arr:\n",
        "    x, y = get_data(idx)\n",
        "    pred = model(x)\n",
        "    loss = loss_fn(pred, y)\n",
        "    epoch_loss += loss.detach().numpy()\n",
        "\n",
        "    print(\"Epoch\", epoch)\n",
        "    print(\"Prediction:\", pred)\n",
        "    print(\"Actual:\", y)\n",
        "    print(\"Loss:\", loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  all_losses.append(epoch_loss)\n",
        "  print(\"Epoch:\", epoch, \"Loss:\", epoch_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "dZFlPKvNO2Na",
        "outputId": "1b741877-8088-4fbd-b458-c6c01e8a4cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No need to download data\n",
            "All letters: 57\n",
            "Rows to process: 20074\n",
            "All names count: 20074\n",
            "All countries count 20074\n",
            "One-hot encoding: tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
            "Shape of embedding: torch.Size([57, 57])\n",
            "Country to index map: {'Arabic': 0, 'Chinese': 1, 'Czech': 2, 'Dutch': 3, 'English': 4, 'French': 5, 'German': 6, 'Greek': 7, 'Irish': 8, 'Italian': 9, 'Japanese': 10, 'Korean': 11, 'Polish': 12, 'Portuguese': 13, 'Russian': 14, 'Scottish': 15, 'Spanish': 16, 'Vietnamese': 17}\n",
            "RNN(\n",
            "  (rnn): RNN(57, 114)\n",
            "  (fc): Linear(in_features=114, out_features=18, bias=True)\n",
            ")\n",
            "Epoch: 0 Loss: 22554.54584160888\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-6f61062b6a5d>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1740\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1741\u001b[0m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Syntax and Parsing\n",
        "\n",
        "This section focuses on the structure and organization of sentences. With these techniques, we can decipher gramatical structures of sentences, which is essential for enabling machines to interpret and generate human language accurately."
      ],
      "metadata": {
        "id": "cLedc2avrCtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parts of Speech Tagging (POS)\n",
        "\n",
        "This is the process of assigning grammatical categories such as nouns, verbs, adjectives, and adverbs to each word in a sentence.\n",
        "\n",
        "In English, here are the categories:\n",
        "\n",
        "1. Noun (NN)\n",
        "2. Verb (VB)\n",
        "3. Adjective (JJ)\n",
        "4. Adverb (RB)\n",
        "5. Pronoun (PRP)\n",
        "6. Preposition (IN)\n",
        "\n",
        "The right way to evaluate this is with \"accuracy\". However, the performance of these taggers can vary significantly depending on several factors:\n",
        "\n",
        "1. Text domain - A tagger trained on news articles may not perform well on social media text because of the language, style, of vocabulary used.\n",
        "2. Language - Some languages may have better taggers compared to others\n",
        "3. Ambiguity - The word \"run\" can be both a nounch and a verg\n",
        "4. Quality of training data"
      ],
      "metadata": {
        "id": "jugd8ZETrXV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"Natural Language Procesing with Python is very fun.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eadeT1tEr-ps",
        "outputId": "cccac80f-ac00-4df6-c13a-530e2d114fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'Language', 'Procesing', 'with', 'Python', 'is', 'very', 'fun', '.']\n",
            "POS Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Procesing', 'VBG'), ('with', 'IN'), ('Python', 'NNP'), ('is', 'VBZ'), ('very', 'RB'), ('fun', 'JJ'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER)\n",
        "\n",
        "This is a subtask of information extraction that aims to identify and classify named entities mentioned within unstructured text.\n",
        "\n",
        "Common categories extraced include:\n",
        "\n",
        "1. Person (PER)\n",
        "2. Organization (ORG)\n",
        "3. Location (LOC)\n",
        "4. Miscellanous (MISC)"
      ],
      "metadata": {
        "id": "5Th-mstTvvjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple is looking to buy a U.K. startup at 2 million dollars\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Named entities:\")\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlnFvVLDwL3r",
        "outputId": "8dafa5bd-67d5-4411-829b-230050ec9c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities:\n",
            "Apple ORG\n",
            "U.K. GPE\n",
            "2 million dollars MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a custom NER\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training import Example\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "# Create a blank English model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Create an NER component and add it to the pipeline\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "ner.add_label(\"GADGET\")\n",
        "\n",
        "# Sample training data\n",
        "TRAIN_DATA = [\n",
        "  (\"Apple is releasing a new iPhone\", {\"entities\": [(25, 32, \"GADGET\")]}),\n",
        "  (\"The new iPad Pro is amazing.\", {\"entities\": [(8, 16, \"GADGET\")]}),\n",
        "]\n",
        "\n",
        "# Transform training format to Spacy format\n",
        "doc_bin = DocBin()\n",
        "examples = []\n",
        "for text, annot in TRAIN_DATA:\n",
        "  doc = nlp.make_doc(text)\n",
        "  example = Example.from_dict(doc, annot)\n",
        "  examples.append(example)\n",
        "\n",
        "# Train the NER model\n",
        "optimizer = nlp.begin_training()\n",
        "for i in range(10):\n",
        "  losses = {}\n",
        "  batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
        "  for batch in batches:\n",
        "    nlp.update(batch, sgd=optimizer, losses=losses)\n",
        "  print(\"Losses:\", losses)\n",
        "\n",
        "# Test the trained model\n",
        "doc = nlp(\"I just bought a new iPhone and an iPad Pro\")\n",
        "print(\"Entities:\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P4HMMOmxK8A",
        "outputId": "82c3afe6-5af3-46cf-872d-30f42a84358a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses: {'ner': 10.333333730697632}\n",
            "Losses: {'ner': 9.872534215450287}\n",
            "Losses: {'ner': 8.998019397258759}\n",
            "Losses: {'ner': 7.608083248138428}\n",
            "Losses: {'ner': 5.530070662498474}\n",
            "Losses: {'ner': 3.4145753979682922}\n",
            "Losses: {'ner': 1.820153184235096}\n",
            "Losses: {'ner': 0.9663957273587584}\n",
            "Losses: {'ner': 6.857765997759998}\n",
            "Losses: {'ner': 2.010009991150582}\n",
            "Entities: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Also called opinion mining. By understanding the sentiment behind text, businesses and organizations can gain valuable insights into public opinion, customer satisfaction an overall sentiment trends.\n",
        "\n",
        "There are 3 main approaches:\n",
        "\n",
        "1. Rule based\n",
        "2. Machine Learning\n",
        "3. Deep learning"
      ],
      "metadata": {
        "id": "Qox-vaQO1JbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rule based Sentiment Analysis\n",
        "\n",
        "This approach relies on manually crafted rules. The steps involved are:\n",
        "\n",
        "1. Tokenization - Split text into words or tokens\n",
        "2. Normalization - Convert to standard form such as lowercase\n",
        "3. **Lexicon lookup** - A sentiment lexicon is used to assign sentiment score to the tokens. Popular sentiment lexicons include afinn, sentiWordNet and NRC Emotion.\n",
        "4. Rule application"
      ],
      "metadata": {
        "id": "ls86ImaF2msm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I love this product! It's amazing.\"\n",
        "print(\"Text:\", text)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "blob = TextBlob(text)\n",
        "print(\"Sentiment:\", blob.sentiment)\n",
        "\n",
        "\"\"\"\n",
        "Polarity scores ranges from -1(very negative) to 1(very positive)\n",
        "Subjectivity scores ranges from 0(objective) to 1(subjective\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfn7x8dr3MIn",
        "outputId": "e07916a6-ae2f-4b34-df01-38d47d85e564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love this product! It's amazing.\n",
            "Sentiment: Sentiment(polarity=0.6125, subjectivity=0.75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Approach\n",
        "\n",
        "This approach involves training models to automatically learn patterns from labeled data. Unlike rule-based approach, which rely on predefined linguistic rules and often struggle with nuanced language, machine learning methods can capture more complex patterns and relationships in data.\n",
        "\n",
        "The analysis typically follow the following steps:\n",
        "\n",
        "1. Data collection - Gathering diverse labeled data\n",
        "2. Data Preprocessing - Tokenizatin, normalization and vectorization\n",
        "3. Feature extraction - Techniques such as TF-IDF, word embeddings or more advanced methods like BERT are used to capture the semantic meaning and context of the text.\n",
        "4. Model Training - Algorithms such as Naive Bayes, Support Vector Machines (SVM) and advanced deep learning techniques such as Convolution Neural Network (CNN) and Recurrent Neural Network (RNN)\n",
        "5. Model Evaluation - accuracy, precision, recall or F1 score\n",
        "6. Prediction\n",
        "\n",
        "Advantages in this approach:\n",
        "\n",
        "1. Better Performance compared to rule-based approach\n",
        "2. Scalable - More suitable on real-world applications and can be trained on large datasets.\n",
        "3. Flexibility - Can be adapted to different domains and languages. Single model can be fine-tuned to different domains\n",
        "\n",
        "However, the limitations of this include:\n",
        "\n",
        "1. Data dependency - Requires large amount of data for training. Without sufficient data, the performance is degraded.\n",
        "2. Complexity - Involves extensive experimentation and parameter optimization, which can be time and resource intensive."
      ],
      "metadata": {
        "id": "2acqPH5Urs2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "  \"I love this product!\",\n",
        "  \"This is the worst service I have ever experienced\",\n",
        "  \"I am very happy with my purchase\",\n",
        "  \"I am disappointed with the quality of this item\"\n",
        "]\n",
        "labels=[1, 0, 1, 0] # Each matching a single sentence in the corpus\n",
        "\n",
        "# Transform text to vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(class_weight=\"balanced\")\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0TXyM79ug6P",
        "outputId": "97e2dc92-cb69-492e-ef29-df7f1281fcfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       1.0\n",
            "           1       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Approach\n",
        "\n",
        "This approach can handle long-range dependencies, large vocabularies and learn hierarchical representations of text, making them particularly powerful for sentiment analysis. LSTM is particularly adept at maintaining context over longer text sequences.\n",
        "\n",
        "Popular architectures include:\n",
        "\n",
        "1. Convolutional Neural Networks (CNNs)\n",
        "2. Recurrent Neural Networs (RNNs)\n",
        "3. Long Short-Term Memory Networks (LSTM)\n",
        "4. Transformer-Based - E.g., BERT"
      ],
      "metadata": {
        "id": "k0KniaZNyZNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTMs approach\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "  \"I love this product!\",\n",
        "  \"This is the worst service I have ever experienced\",\n",
        "  \"I am very happy with my purchase\",\n",
        "  \"I am disappointed with the quality of this item\"\n",
        "]\n",
        "labels=[1, 0, 1, 0] # Each matching a single sentence in the corpus\n",
        "\n",
        "# Tokenize and pad the text data\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100) # Assume the longest sentence is 10 words long\n",
        "\n",
        "# Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Define the LSTM\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Predict the sentiment of new text\n",
        "new_text = [ \"This product is excellent and I love it.\" ]\n",
        "new_text_seq = tokenizer.texts_to_sequences(new_text)\n",
        "new_text_padded = pad_sequences(new_text_seq, maxlen=10)\n",
        "prediction = model.predict(new_text_padded)\n",
        "print(\"Prediction:\", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhWF-0hOzRzq",
        "outputId": "bf9353fd-93d7-43cb-c88e-fd7f0134cd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.6667 - loss: 0.6888 - val_accuracy: 0.0000e+00 - val_loss: 0.7048\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.6667 - loss: 0.6726 - val_accuracy: 0.0000e+00 - val_loss: 0.7217\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.6667 - loss: 0.6560 - val_accuracy: 0.0000e+00 - val_loss: 0.7431\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.6667 - loss: 0.6376 - val_accuracy: 0.0000e+00 - val_loss: 0.7726\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.6667 - loss: 0.6165 - val_accuracy: 0.0000e+00 - val_loss: 0.8164\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.0000e+00 - loss: 0.8164\n",
            "Loss: 0.8163905143737793\n",
            "Accuracy: 0.0\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step\n",
            "Prediction: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using BERT\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "  \"I love this product!\",\n",
        "  \"This is the worst service I have ever experienced\",\n",
        "  \"I am very happy with my purchase\",\n",
        "  \"I am disappointed with the quality of this item\"\n",
        "]\n",
        "labels=[1, 0, 1, 0] # Each matching a single sentence in the corpus\n",
        "\n",
        "# Tokenize and pad the text data\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "X = tokenizer(corpus, padding=True, truncation=True, return_tensors=\"tf\", max_length=10)\n",
        "\n",
        "# Convert the TensorFlow tensor to a NumPy array before splitting\n",
        "input_ids = X[\"input_ids\"].numpy() # Convert to NumPy array\n",
        "\n",
        "# Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_ids, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the BERT model\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, np.array(y_train), epochs=3, batch_size=5, validation_data=(x_test, np.array(y_test)))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, np.array(y_test))\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Predict the sentiment of new text\n",
        "new_text = [ \"This product is excellent and I love it.\" ]\n",
        "new_text_enc = tokenizer(new_text, padding=True, truncation=True, return_tensors=\"tf\", max_length=10)\n",
        "prediction = model.predict(new_text_enc[\"input_ids\"])\n",
        "print(\"Logits:\", np.argmax(prediction.logits))\n",
        "print(\"Prediction:\", \"Positive\" if np.argmax(prediction.logits) == 1 else \"Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wzxl8sp20ll",
        "outputId": "3228cd01-07c6-4eaa-85fe-1b00d8f7d273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1/1 [==============================] - 84s 84s/step - loss: 0.8308 - accuracy: 0.0000e+00 - val_loss: 1.3334 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5998 - accuracy: 0.6667 - val_loss: 0.6890 - val_accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9291 - accuracy: 0.0000e+00 - val_loss: 2.8159 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 2.8159 - accuracy: 0.0000e+00\n",
            "Loss: 2.815908908843994\n",
            "Accuracy: 0.0\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "Logits: 1\n",
            "Prediction: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "A technique to automatically identify the underlying topics present in a collection of documents. This helps in organizing, understanding, and summarizing large datasets by discovering the hidden thematic structure within the text.\n",
        "\n",
        "Various techniques are used, including:\n",
        "\n",
        "1. Latent Semantic Analysis (LSA)\n",
        "2. Latent Dirichlet Allocation (LDA)\n",
        "3. Hierarchical Dirichlet Process (HDP)"
      ],
      "metadata": {
        "id": "MlDRCyUZXdAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Semantic Analysis\n",
        "\n",
        "This is a foundational technique in the field of topic modeling and information retrieval that has been extensively studied and applied in various domains.\n",
        "\n",
        "It works by reducing the dimensionality of the text data, which involves transforming original term-document matrix into a lower-dimensional space. This transformation is achieved through a mathematical process known as Singular Value Decomposition (SVD), which decomposes the matrix into several component matrices.\n",
        "\n",
        "The steps involved are:\n",
        "\n",
        "1. Create a term-document matrix - Represent the text data as a matrix, each row corresponds to a term (word), each column corresponds to a document, each entry represents frequency of the term in respective document.\n",
        "2. Apply SVD (Singular Value Decomposition)\n",
        "3. Reduce dimensionality\n",
        "4. Interpret topics\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1. Dimensionality Reduction\n",
        "2. Captures Synonym\n",
        "3. Noise Reduction\n",
        "4. Enhanced Information Retrieval\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "1. Linear Assumption -\n",
        "2. Interpretability\n",
        "3. Computationally Intensive\n",
        "4. Limited Context Understanding\n"
      ],
      "metadata": {
        "id": "K8H3soQfX1AV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "  \"The cat sat on the mat\",\n",
        "  \"The dog is chasing the cat\",\n",
        "  \"The mat is on the table\",\n",
        "  \"The cat is sitting on the mat\"\n",
        "  \"The cat is chasing the dog\"\n",
        "]\n",
        "\n",
        "# Create a TD-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Apply SVD\n",
        "lsa = TruncatedSVD(n_components=2, random_state=42)\n",
        "X_reduced = lsa.fit_transform(X)\n",
        "\n",
        "# Print the terms and their corresponding components\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i, comp in enumerate(lsa.components_):\n",
        "  terms_comp = zip(terms, comp)\n",
        "  sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)[:10]\n",
        "  print(\"Topic\", i)\n",
        "  for term, weight in sorted_terms:\n",
        "    print(f\"- {term}: {weight:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4eK4LY5ahNt",
        "outputId": "78cb4e7f-ed08-4a69-aa9e-6b446420386a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0\n",
            "- the: 0.6749\n",
            "- is: 0.3495\n",
            "- cat: 0.3495\n",
            "- on: 0.2639\n",
            "- mat: 0.2411\n",
            "- chasing: 0.2263\n",
            "- dog: 0.2263\n",
            "- sat: 0.1529\n",
            "- table: 0.1529\n",
            "- matthe: 0.1076\n",
            "Topic 1\n",
            "- mat: 0.5070\n",
            "- table: 0.3215\n",
            "- sat: 0.3215\n",
            "- on: 0.3088\n",
            "- the: 0.0778\n",
            "- matthe: -0.1593\n",
            "- sitting: -0.1593\n",
            "- is: -0.2085\n",
            "- cat: -0.2085\n",
            "- dog: -0.3854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "Unlike LSA, which relies on mathematical foundations rooted in linear algebra, LDA is a generative probabilistic model that aims to uncover the hidden thematic strucure in a collection of documents by assuming a statistical framework.\n",
        "\n",
        "The core assumption is that documents are a mixture of various topics, and each topic, itself is a mixture of words with certain probabilities. By employing LDA, researchers and practitioners can discover the underlying topics that best explain the observed documents, which helps in understanding the thematic composition of large text corpora.\n",
        "\n",
        "Can be used in the following areas:\n",
        "\n",
        "1. Document classification\n",
        "2. Recommendation systems\n",
        "3. Gaining insights from massive datasets in fields like social sciences and digital humanities.\n",
        "\n",
        "LDA offers significant advantages in terms of its probabilistic foundation, flexibility and interpretability, making it a powerful tool for topic modeling. However, it also faces limitations related to scalability, hyperparameter tuning, and the validity of its underlying assumptions."
      ],
      "metadata": {
        "id": "WNu79UZS9T1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pprintpp\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from pprintpp import pprint\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "  \"The cat sat on the mat\",\n",
        "  \"The dog is chasing the cat\",\n",
        "  \"The mat is on the table\",\n",
        "  \"The cat is sitting on the mat\"\n",
        "  \"The cat is chasing the dog\"\n",
        "]\n",
        "\n",
        "# Tokenize the text and remove stop words\n",
        "texts = [[word for word in doc.lower().split()] for doc in corpus]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Create a bag-of-words representation of the documents\n",
        "corpus_bow = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = LdaModel(corpus=corpus_bow, id2word=dictionary, num_topics=2, random_state=42, passes=10)\n",
        "\n",
        "# Print the topics and their associated words\n",
        "pprint(lda_model.print_topics(num_words=5))\n",
        "\n",
        "# Assign topics to a new document\n",
        "new_doc = \"The cat is chasing the dog\"\n",
        "new_doc_bow = dictionary.doc2bow(new_doc.lower().split())\n",
        "print(lda_model.get_document_topics(new_doc_bow))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH_DIuZK-NvZ",
        "outputId": "98db03b0-c963-49b6-cba3-793fcdc7c717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pprintpp\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: pprintpp\n",
            "Successfully installed pprintpp-0.4.0\n",
            "[\n",
            "    (\n",
            "        0,\n",
            "        '0.269*\"the\" + 0.127*\"is\" + 0.127*\"cat\" + 0.099*\"on\" + 0.070*\"chasing\"',\n",
            "    ),\n",
            "    (\n",
            "        1,\n",
            "        '0.094*\"the\" + 0.093*\"mat\" + 0.092*\"sat\" + 0.092*\"on\" + 0.091*\"table\"',\n",
            "    ),\n",
            "]\n",
            "[(0, 0.92085874), (1, 0.07914128)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical Dirichlet Process (HDP)\n",
        "\n",
        "This is an extension of LDA. It removes the necessity of specifying the number of topics in advance. Instead, HDP automatically determines the appropriate number of topics based on the data it analyzes."
      ],
      "metadata": {
        "id": "znzlhAAQAgx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pprintpp\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import HdpModel\n",
        "from pprintpp import pprint\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "  \"The cat sat on the mat\",\n",
        "  \"The dog is chasing the cat\",\n",
        "  \"The mat is on the table\",\n",
        "  \"The cat is sitting on the mat\"\n",
        "  \"The cat is chasing the dog\"\n",
        "]\n",
        "\n",
        "# Tokenize the text and remove stop words\n",
        "texts = [[word for word in doc.lower().split()] for doc in corpus]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Create a bag-of-words representation of the documents\n",
        "corpus_bow = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Train the LDA model\n",
        "hdp_model = HdpModel(corpus=corpus_bow, id2word=dictionary)\n",
        "\n",
        "# Print the topics and their associated words\n",
        "pprint(hdp_model.print_topics(num_topics=4, num_words=5))\n",
        "\n",
        "# Assign topics to a new document\n",
        "new_doc = \"The cat is chasing the dog\"\n",
        "new_doc_bow = dictionary.doc2bow(new_doc.lower().split())\n",
        "print(hdp_model[new_doc_bow])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7y9JiYSBIu2",
        "outputId": "15c5b24d-efc1-4a5d-c220-5e9e2bf47985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pprintpp in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "[\n",
            "    (0, '0.238*sitting + 0.211*the + 0.155*mat + 0.154*on + 0.090*dog'),\n",
            "    (1, '0.255*sat + 0.214*the + 0.190*is + 0.073*cat + 0.061*dog'),\n",
            "    (2, '0.306*the + 0.199*sat + 0.166*matthe + 0.131*on + 0.064*dog'),\n",
            "    (3, '0.410*sitting + 0.179*mat + 0.152*the + 0.127*table + 0.039*on'),\n",
            "]\n",
            "[(0, 0.05881033096301249), (1, 0.8810314306848641), (2, 0.021141745942392966), (3, 0.013692257035095827)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization\n",
        "\n",
        "The primary object is to retain the most important information and key points while significantly reducing the amont of text that needs to be read.\n",
        "\n",
        "Can be broadly classified into 2 categories:\n",
        "\n",
        "1. Extractive summarization\n",
        "2. Abstractive summarization"
      ],
      "metadata": {
        "id": "lnnblUTMCAS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extractive Summarization\n",
        "\n",
        "This involves selecting the most important sentences from the original text and combining them to form a summary. The steps involved are:\n",
        "\n",
        "1. Preprocessing - Tokenization, stop word removeal, normalization\n",
        "2. Sentence scoring - Term frequence, sentence position and similarity to title\n",
        "3. Sentence selection\n",
        "4. Summary generation\n",
        "\n",
        "This method is straightforward but comes with limitations. The biggest is that the resulting summary may lack coherence and fluency. Additionally, it doesn't generate new sentences or paraphrase content.\n",
        "\n",
        "In addition to the simple term frequency method, there are more advanced techniques for extractive summarization, including:\n",
        "\n",
        "1. TextRank - A graph-based ranking algorithm that uses sentence similarity to rank sentences\n",
        "2. Latent Semantic Analysis (LSA) - An unsupervised learning technique that captures the latent structure of the text and identifies key sentences.\n",
        "3. Supervised learning - Using labeled data to train a machine learning model to score and select sentences for summarization."
      ],
      "metadata": {
        "id": "pjRxptiUCbOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstractive Summarization\n",
        "\n",
        "This is a more advanced and sophisticated technique. It involves generating new sentences that effectively convey the meaning of the original text. It goes beyond simply selecting key sentences.\n",
        "\n",
        "This techniques involve 2 main components that work together to transform lengthy input text into a concise summary.\n",
        "\n",
        "1. Encoder - Convert the input text into a fixed-size context vector.\n",
        "2. Decoder - Generate the summary\n",
        "\n",
        "Various models such as RNN, LSTM and Transformer-based models are among the most commonly used architectures.\n",
        "\n",
        "For advanved scenarios, varios techniques can be used, such as:\n",
        "\n",
        "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
        "2. GPT (Generative Pre-trained Transformer)\n",
        "3. BART (Bidirectional and Auto-Regressive Transformers)"
      ],
      "metadata": {
        "id": "zLD1wM2cD51D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Load the pre-trained BART model and tokenizer\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Sample text to summarize\n",
        "text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language.\n",
        "It is a field focused on enabling computers to understand, interpret, and generate human language.\n",
        "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and machine translation.\"\"\"\n",
        "\n",
        "# Tokenize and encode the text\n",
        "inputs = tokenizer.encode(f\"summarize: {text}\", return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "# Generate the summary\n",
        "summary_ids = model.generate(inputs, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Summary: {summary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWZedhw4FplZ",
        "outputId": "3f01824d-20fd-4cb0-c5f9-9c46d4f10f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence. It is a field focused on enabling computers to understand, interpret, and generate human language.Challenges in natural language processing frequently involve speech recognition and machine translation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qBO9XEvUJeAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation\n",
        "\n"
      ],
      "metadata": {
        "id": "lxR8dRH_JeWK"
      }
    }
  ]
}